# AAAI 2019 | 基于分层强化学习的关系抽取

现有的关系抽取方法大多是先识别所有实体然后再确定关系类型。但是这类方法并没有考虑到实体与关系之间的作用。**本文应用分层强化学习框架来增强实体提及和关系类型之间的交互，将相关实体视为关系的集合。**此外，该方法还解决了抽取重叠关系（Overlapping Relations）的问题。

![image-20231030180711186](https://picgo-1305561115.cos.ap-beijing.myqcloud.com/img/image-20231030180711186.png)

论文链接：

源码链接：

## 研究动机

**该研究主要解决的问题有两个：**

**1. 大部分现有的方法在实体被识别后才决策关系类型。**这种方法存在两个弊端：一是并没有充分挖掘实体和关系之间的联系，而是把他们割裂作为两个子任务去处理；二是很多和关系无关的实体会带来噪声；

**2. 关系抽取会存在重叠关系问题（也叫一对多问题）。**在一句话中，一个实体可能会存在多个关系，或者一个实体对可能存在多种关系。目前已知只有 CopyR 方法研究了这个问题，但是本文作者实验证明了这种方法严重依赖数据，并且无法抽取多词语关系。 如图：

![image-20231030180901307](https://picgo-1305561115.cos.ap-beijing.myqcloud.com/img/image-20231030180901307.png)

## **层次抽取框架**

首先，文章定义了“关系指示符”（Relation Indicator）。 当在一句话中的某个位置有足够信息去识别语义关系时，我们把这个位置就叫做“关系指示符”。它可以是名词、动词、介词，或者是一些其他的符号比如逗号、时间等等。关系指示符在本结构中非常重要，因为整个的关系抽取任务可以分解为“关系指示符”和“关系中的实体抽取”。

整体来看，**关系抽取过程如下：**

一个 agent 在扫描句子时预测特定位置的关系类型。不同于识别实体对之间关系的关系分类，该过程不需要对实体进行标注。当在一个时间步中没有足够的信息来指示语义关系时，agent 可以选择 NR，这是一种指示没有关系的特殊关系类型。否则，触发一个关系指示符，agent 启动一个用于实体提取的子任务，以识别两个实体之间的关系。当实体被识别时，子任务完成，代理继续扫描句子的其余部分寻找其他关系。

**这种过程可以被表述为半马尔可夫决策过程：**1）检测句子中关系指示符的高级 RL 过程；2）识别对应关系的相关实体的低级 RL 过程。

通过将任务分解成两个 RL 过程的层次结构，该模型有利于处理对于同一实体对具有多种关系类型的句子，或者一个实体涉及多种关系的情况。过程如图：

![image-20231030181000980](https://picgo-1305561115.cos.ap-beijing.myqcloud.com/img/image-20231030181000980.png)

下面分别介绍两个决策过程。

**Relation Detection with High-level RL**

High-level RL 的策略（policy）µ 旨在从句子中找到存在的关系，可以看做是带有 options 的 RL policy。option 指的是一旦 agent 执行了某个选项，就会启动低级别的 RL 策略。

**Option：**option 在集合 O = {NR} ∪ R 中选择，当 low-level RL 进入结束状态，agent 的控制将被 high-level 接管去执行下一个 option。

**State：**状态 S 由以下三者共同决定：当前的隐状态 $h_t$，最后一个 option 的 relation type vector $ v_t^r$和上一个时间步的状态$s_{t-1}$ 。公式如下：

<img src="https://picgo-1305561115.cos.ap-beijing.myqcloud.com/img/image-20231030181223625.png" alt="image-20231030181223625" style="zoom:50%;" />

 $f^h$是非线性变换， $h_t$是由 Bi-LSTM 得到隐状态。

**Policy：**关系检测的策略，也就是 option 的概率分布，如下，其中 W 是权重：

<img src="https://picgo-1305561115.cos.ap-beijing.myqcloud.com/img/image-20231030181338878.png" alt="image-20231030181338878" style="zoom:50%;" />

**Reward**：环境提供给 Agent 的一个可量化的标量反馈信号，也就是 reward。reward 计算方法如下：

<img src="https://picgo-1305561115.cos.ap-beijing.myqcloud.com/img/image-20231030181541506.png" alt="image-20231030181541506" style="zoom:50%;" />

最后，用一个最终的 reward 来评价句子级别的抽取效果：

<img src="https://picgo-1305561115.cos.ap-beijing.myqcloud.com/img/image-20231030181632575.png" alt="image-20231030181632575" style="zoom:50%;" />

**Entity Extraction with Low-level RL**

当 High-level RL policy 预测了一个非 NR 的relation，Low-level RL 会抽取 relation 中的实体。High-level RL 的 option 会作为 Low-level RL 的额外输入。

**Action：**action 会给当期的词分配一个 tag，tag 包括 A=({S,T,O}×{B,I})∪{N}。其中，S 是参与的源实体，T 是目标实体，O 是和关系无关的实体，N 是非实体单词，B 和 I 表示一个实体的开始和结束。可参看下图：

![image-20231030181749803](https://picgo-1305561115.cos.ap-beijing.myqcloud.com/img/image-20231030181749803.png)

**State：**类似 High-level RL 中的关系检测，High-level 中的状态计算方法如下：

<img src="https://picgo-1305561115.cos.ap-beijing.myqcloud.com/img/image-20231030181848496.png" alt="image-20231030181848496" style="zoom:50%;" />

$h_t$是当前单词的隐状态，同样也是经过 Bi-LSTM 计算得到，$v^e_t$ 是可学习的实体标签向量， $s_{t-1}$是上一阶段的状态（注意，既可以是 High-level 的状态，也可以是 Low-level 的状态）。g 和 f 都是多层感知机。

**Policy：**由句子到实体的概率计算如下：

<img src="https://picgo-1305561115.cos.ap-beijing.myqcloud.com/img/image-20231030182530225.png" alt="image-20231030182530225" style="zoom:50%;" />